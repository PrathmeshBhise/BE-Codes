{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDWPp4DD22aefWaXv5euX5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SoT5qUdByMWl","executionInfo":{"status":"ok","timestamp":1730994022461,"user_tz":-330,"elapsed":424,"user":{"displayName":"PRATHMESH BHISE","userId":"10624294756255944730"}},"outputId":"5370ac21-5ec6-47e6-d06d-8083a35b07b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Converged after 63 iterations.\n","Local minimum occurs at x = -2.999995096014269\n","Minimum value of the function is y = 2.4049076048192486e-11\n"]}],"source":["# Define the function and its gradient\n","def function(x):\n","    return (x + 3)**2\n","\n","def gradient(x):\n","    return 2 * (x + 3)\n","\n","# Parameters for Gradient Descent\n","x = 2  # Starting point\n","alpha = 0.1  # Learning rate\n","iterations = 100  # Number of iterations\n","tolerance = 1e-6  # Convergence criterion\n","\n","# Gradient Descent loop\n","for i in range(iterations):\n","    grad = gradient(x)  # Calculate the gradient\n","    new_x = x - alpha * grad  # Update x\n","\n","    # Check for convergence\n","    if abs(new_x - x) < tolerance:\n","        print(f\"Converged after {i+1} iterations.\")\n","        break\n","\n","    x = new_x  # Update x to the new value\n","\n","print(f\"Local minimum occurs at x = {x}\")\n","print(f\"Minimum value of the function is y = {function(x)}\")\n"]}]}